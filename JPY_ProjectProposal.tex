\documentclass{report}

\input Latex_macros/Definitionen.tex

\usepackage{a4}
\usepackage[all]{xy}
\usepackage{enumerate}
\usepackage{xr}
\def\p{{\rm p}}
\def\E{{\rm E}}
\def\P{{\rm P}}


\begin{document}
	
	\title{Initial Document\\Heuristics for SAT solvers}
	\author{Ji Pengyun\\
		\href{http://www.swan.ac.uk/compsci/}{Computer Science Department}, \href{http://www.swan.ac.uk/science/}{College of Science}\\
		\href{http://www.swan.ac.uk/}{Swansea University, UK}\\
		%{\small \url{http://www.swan.ac.uk/NOT_YET}}
	}
	
	\maketitle
	
	\tableofcontents
	
	\chapter{General plans}
	\label{cha:Generalpreparations}
	
	
	To accomplish the initial report, planning uses ``Todos'':
	\begin{enumerate}
		\item To meet Oliver twice a week to communicate:
		\begin{enumerate}[Week 1]
			\item 21/3/2016 - 27/3/2016: XXX
		\end{enumerate}
		\item Integrate Latex-structure, either by appropriate copying or by using directly Annotations.
		\item Looking into the paper ``Fundaments of Branching Heuristics'' of Handbook Chapter 7 (O.Kullmann, 2009). Start with
		\begin{enumerate}
			\item Missing proofs of Theorems:
			\begin{enumerate}
				\item XXX
			\end{enumerate}
		\end{enumerate}
		\item Working through Chapter 7:
		\begin{enumerate}
			\item {\bf The Definitions of handbook chapter.}
			
			{\defi $\mathcal{BT} :=\bigcup_{k\in \mathbb{N}}(\mathbb{R}_{>0})^k$}
			denotes the set of {\bf branching tuples}.
			
			Remarks:
			
			\begin{enumerate}
				\item  Basic measurements for branching tuples are minimum $\min: \mathcal{BT} \to \mathbb{R}_{>0}$,
				maximum $\max : \mathcal{BT} \to \mathbb{R}_{>0}$, sum $\Sigma : \mathcal{BT} \to \mathbb{R}_{>0}$, and width $|| : \mathcal{BT} \to \mathbb{N}$.
				The minimum $\min(a)$ of a branching tuple is a ``worst-case view'', while
				max($a$) is a ``best-case view''. In general, disregarding the values, the
				larger $|a|$, i.e., the wider the branching is, the worse it is.
				
				\item The set of branching tuples of width $k$ is ${\mathcal{BT}}^{{(k)}}:= \{t \in \mathcal{BT} : |t| = k\}$,
				which is a {\it cone}, that is for $a \in \mathcal{BT}^{(k)}$ and $\lambda \in \mathbb{R}_{>0}$ we have $\lambda \cdot t \in \mathcal{BT}^{(k)} $, and for $a,b \in \mathcal{BT}^{(k)}$ we have $a + b \in \mathcal{BT}^{(k)}$.
				
				\item Branching tuples of width 1, which do not represent ``real branchings'' but
				``reductions'', are convenient to allow.
				
				\item One could also allow the empty branching tuple as well as the zero branching tuple (0) (of width 1), but for the sake of simplicity we abstain from
				such systematic extensions here.
			\end{enumerate}
			
			
			{\defi Define $\chi _k : \mathcal{BT} \times \mathbb{R}_{>0} \to \mathbb{R}_{>0}$ by $\chi (t,x) :=
				\sum^{|t|}_{i=1} x^{-t_i}$. Observe
				that for each $t \in \mathcal{BT}$ the map $\chi (t,-) : \mathbb{R}_{>0} \to \mathbb{R}_{>0}$ is strictly decreasing with
				$\chi (t,1) = |t| \geq 1$ and $\lim _{x\to \infty} \chi (t,x) = 0$. Now $\tau : \mathcal{BT} \to \mathbb{R}_{\geq 1} $ is defined as the
				unique $\tau (t) := x_0 \in\mathbb{R}_{\geq 1} $ such that $\chi (t)(x_0 ) = 1$ holds.}
			
			By definition we have $\tau (t) \geq 1$, with $\tau (t) = 1 \Leftrightarrow |t| = 1$. For $k \in \mathbb{N} $ we denote
			by $\tau_k : \mathcal{BT}^{(k)}
			\to \mathbb{R}_{\geq 1} $ the $\tau $-function restricted to branching tuples of width $k$.
			
			
			{\lem For every $a \in \mathcal{BT} , k \in \mathbb{N}$ and $\lambda \in \mathbb{R}_{>0}$ we have:
				
				\begin{enumerate}
					\item[1.]$\tau (\lambda \cdot a) = \tau (a)^{1/\lambda} $.
					
					\item[2.] $\tau_k (\vec{1}) = k$.
					
					\item[3.] $\tau_k$ for $k \geq 2$ is strictly decreasing in each component.
					
					\item[4.] $\tau_k$ is symmetric, that is, invariant under permutation of branching tuples.
					
					\item[5.] $\tau (a)^{\min(a)} \leq |a| \leq \tau (a)^{\max(a)}$, that is, $|a|^{1/\max(a)} \leq \tau (a) \leq |a|^{1/\min(a)}$.
					
					\item[6.] $\lim_{\lambda \to 0} \tau (a;(\lambda )) = \infty$ and $\lim _{\lambda \to \infty }\tau (a;(\lambda )) = \tau (a)$.
				\end{enumerate}}
				
				The $\tau $-function fulfils powerful convexity properties, from which non-trivial
				further properties will follow. A function $f : C \to \mathbb{R}$ defined on some convex
				subset $C \subseteq \mathbb{R}^k$ is called ``strictly convex'' if for all $x,y \in C$ and $0 < \lambda < 1 $ holds
				$f(\lambda x+(1-\lambda )y) < \lambda f(x)+(1-\lambda )f(y)$; furthermore $f$ is called ``strictly concave''
				if $-f$ is strictly convex. By definition $\tau_1$ is just the constant function with value
				1, and so doesn't need to be considered here.
				
				{\bf Bounds on the tau-function}
				
				Just from being symmetric and strictly convex it follows, that $\tau_k (a)$ for tuples a
				with a given fixed sum $\Sigma (a) = s$ attains its strict minimum for the constant tuple
				(with entries $\frac sk $); Thus, using $\mathfrak{A}(t) :=
				\sum(t)/|t|$ for the arithmetic mean of a branching tuple, we have $\tau (\mathfrak{A}(t) \cdot \vec{1}) \leq \tau (t)$ (with
				strict inequality iff $t$ is not constant).
				
				\begin{enumerate}
					\item The {\it arithmetic mean}, the {\it geometric mean}, and the {\it harmonic mean} of
					branching tuples $t$ are denoted respectively by $\mathfrak{A}(t) :=\frac1{|t|} \Sigma (t) =
					\frac1{|t|}\sum^{|t|}_{i=1} t_i$,
					$\mathfrak{G}(t) :=\sqrt[|t|]{\prod^n_{i=1} t_ i}$
					and $\mathfrak{H}(t) := |t|/(\sum^n_{i=1}\frac1{t_i})$. We recall the well-known
					fundamental inequality between these means: $\mathfrak{H}(t) \leq \mathfrak{G}(t) \leq \mathfrak{A}(t)$ (where
					strict inequalities hold iff $t$ is not constant).
					
					\item More generally we have the {\it power means} for $\alpha \in \overline{\mathbb{R}}$ given by $\mathfrak{M} _\alpha (t) :=(\frac1{|t|}\sum^{|t|}_{i=1} t^\alpha_i)^{1/\alpha}$
					for $\alpha \notin \{-\infty ,0,+\infty \}$, while we set $\mathfrak{M} _{-\infty} (t) := \min(t)$,
					$\mathfrak{M}_ 0 (t) := \mathfrak{G}(t)$ and $\mathfrak{M} _{+\infty} (t) := \max(t)$. By definition we have $\mathfrak{M}_{{-1}} (t) =
					\mathfrak{H}(t)$ and $\mathfrak{M}_ 1 (t) = \mathfrak{A}(t)$. In generalisation of the above fundamental inequality we have for $\alpha $, $\alpha ' \in \overline{\mathbb{R}}$ with $\alpha < \alpha '$ the inequality $\mathfrak{M} _\alpha (t) \leq \mathfrak{M} _{\alpha ' } (t)$,
					which is strict iff $t$ is not constant.
				\end{enumerate}
				{\defi Consider $k \in \mathbb{N}$. A {\bf mean} is a map $M : \mathcal{BT}^{(k)}
					\to\mathbb{R}_{ >0}$
					which is continuous, strictly monotonic increasing in each coordinate, symmetric
					(i.e., invariant under permutation of the arguments) and ``consistent'', that is,
					$\min(a) \leq M(a) \leq \max(a)$ for $a \in \mathcal{BT}^{(k)}$. A mean $M$ is {\bf homogeneous} if $M $ is
					positive homogeneous, i.e., for $\lambda \in \mathbb{R}_{ >0}$ and $a \in \mathcal{BT}^{(k)}$
					we have $M(\lambda \cdot a) = \lambda \cdot M(a)$.}
				
				All power means are homogeneous means. Yet $k$-ary means $M$ are only
				defined for tuples of positive real numbers $a \in \mathbb{R}_{ >0}^ k
				$, and we extend this as follows
				to allow arguments 0 or $+\infty $, using the extended real line $\overline{\mathbb{R}} = {\mathbb{R}} \cup \{\pm \infty \}$. We
				say that $M$ is defined for $a \in \overline{\mathbb{R}}_{ \leq0}^ k$
				(allowing positions to be 0 or $+\infty $) if the
				limit $\lim _{a ' \to a,a' \in \mathbb{R}_{>0}^k} M(a')$ exists in $\overline{\mathbb{R}}$, and we denote this limit by $M(a)$ (if the
				limit exists, then it is unique). Power means $ \mathfrak{M} _\lambda (a)$ for $\lambda \neq0$ are defined for all
				$a \in \overline{\mathbb{R}}_{ \leq0}^ k$, while $\mathfrak{M}_ 0 (a) = \mathfrak{G}(a)$ is defined iff there are no indices $i,j $ with $a_i= 0$
				and $a _j = \infty $.
				
				{\defi Consider a mean $M : \mathcal{BT}^{(k)}
					\to {\mathbb{R}}_{>0}$. We say that $M$ is $\infty ${\bf -dominated} resp. 0{\bf -dominated} if for every $a \in \overline{\mathbb{R}}_{ \geq0}^ k$, such that an index $i$ with
					$a _i = \infty$ resp. $a_ i = 0$ exists, $M(a)$ is defined with $M(a) = \infty$ resp. $M(a) = 0$. On
					the other hand, $M$ {\bf ignores} $\infty$ resp. {\bf ignores} 0 if $M(a;(\infty ))$ resp. $M(a;(0))$ is
					defined iff $M(a)$ is defined with $M(a;(\infty )) = M(a)$ resp. $M(a;(0)) = M(a)$.}
				
				
				Power means $\mathfrak{M} _\lambda$ with $\lambda > 0$ are $\infty $-dominated and ignore 0, while for $\lambda < 0$
				they are 0-dominated and ignore $\infty $. The borderline case $\mathfrak{M}_ 0 = \mathfrak{G}$ is $\infty $-dominated
				as well as 0-dominated if only tuples are considered for which $\mathfrak{G}$ is defined (and
				thus we do not have to evaluate ``$0 \cdot \infty $'').
				
				Another important properties of means is convexity resp. concavity. Power
				means $\mathfrak{M }_\alpha$ with $\alpha > 1$ are strictly convex, while power means $\mathfrak{M}_\alpha$ with $\alpha < 1$ are
				strictly concave; the borderline case $\mathfrak{M}_1 = \mathfrak{A}$ is linear (convex and concave).
				
				{\lem For every concave mean $M$ we have $\mathfrak{M} \leq \mathfrak{A}$.}
				
				{\prf By Jensen's inequality we have $M(a) =\sum_{\pi \in S_k}\frac1{k!} M(\pi * a) \leq
					M( \sum_{\pi \in S_k}\frac1{k!}\cdot (\pi * a)) = M(\mathfrak{A}(a) \cdot \vec{1}) = \mathfrak{A}(a)$.} \hfill  $\Box$
				
				{\bf Jensen's Inequality}
				
				If $p_1,\ldots,p_n$ are positive numbers which sum to 1 and $f$ is a real
				continuous function that is convex, then
				\begin{equation}
				f\left(\sum^n_{i=1}p_ix_i\right) \leq \sum^{n}_{i=1}p_if(x_i).
				\end{equation}
				
				If $f$ is concave, then the inequality reverses, giving
				\begin{equation}
				f\left(\sum^n_{i=1}p_ix_i\right) \geq \sum^{n}_{i=1}p_if(x_i).
				\end{equation}
				
				The special case of equal $p_i=1/n$ with the concave function $\ln x$ gives
				\begin{equation}
				\ln\left(\frac1n\sum^n_{i=1}x_i\right) \geq \frac1n\sum^{n}_{i=1}\ln x_i,
				\end{equation}
				
				
				which can be exponentiated to give the arithmetic mean-geometric mean inequality
				\begin{equation}
				\frac{x_1+x_2+\ldots + x_n}{n}\geq \sqrt[n]{x_1x_2\cdots x_n}.
				\end{equation}
				
				Here, equality holds iff $x_1=x_2=\ldots = x_n$.
				
				
				
				
			\end{enumerate}
			
			{\bf Associating probability distributions with branching tuples}
			
			{\defi Given a branching tuple $a = (a_1 ,\ldots,a_k )$, the branching tuple
				$\tau^\p (a) \in \mathcal{BT}^{(k)}$ is defined by $\tau^\p (a) _i := \tau (a) ^{-a_i}$ for $i \in \{1,\ldots,k\}$.}
			
			Remarks:
			
			\begin{enumerate}
				\item By definition we have $\Sigma (\tau^\p (a)) = 1$, and thus $\tau^\p (a)$ represents a probability distribution (on $\{1,\ldots,k\}$).
				
				\item  For $\lambda \in {\mathbb{R}}_{>0}$ we have $\tau^\p (\lambda \cdot a) = \tau^\p (a)$, and thus $\tau^\p (a) $ only depends on
				the relative sizes of the entries $a_ i$, and not on their absolute sizes.
				
				\item  For fixed $k$ we have the map $\tau^p_k: \mathcal{BT}^{(k)}\to {\rm int}(\sigma_{k-1} ) \subset \mathcal{BT}^{(k)}$
				from the set
				of branching tuples $(a_1 ,\ldots,a_k )$ of length $k$ to the interior of the $(k - 1)$-dimensional standard simplex $\sigma_{k-1}$, that is, to the set of all branching
				tuples $(p_1 ,\ldots,p_k ) \in \mathcal{BT}^{(k)}$
				with $p_1 + \cdots + p_k = 1$. We have already seen
				that $\tau^p_k (\lambda \cdot a) = \tau^p_k (a)$ holds. Furthermore $\tau^p_k$
				is surjective, i.e., every probability distribution of size $k$ with only nonzero probabilities is obtained,
				with $(\tau^p_k )^{-1} ((p_1 ,\ldots,p_k )) = {\mathbb{R}}_{>0}\cdot (-\log(p_1 ),\ldots,-\log(p_k ))$.
			\end{enumerate}
			
			{\bf Notions for trees}
			
			For a node $v \in V (T)$ let $d_T (v)$ be the {\it depth} of $v$ (the length of the path from
			the root to $v$), and for $i \in \{0,\ldots,{\rm d}(v)\}$ let $T(i,v)$ be the vertex with depth $i$ on
			the path from the root to $v$ (so that $T(0,v) = {\rm rt}(T) $ and $T({\rm d}(v),v) = v)$. For a
			node $v \in V (T)$ we denote by $T_v$ the subtree of $T$ with root $v$. Before turning to
			our main subject, measuring the size of trees, some basic strategic ideas should
			be pointed out:
			\begin{itemize}
				\item Trees are considered as ``static'', that is, as given, not as evolving; the
				main advantage of this position is that it enables us to consider the ``real''
				backtracking trees, in contrast to the standard method of ignoring the real
				object and only to consider approximations given by recursion equations.
				
				\item The number of leaves is a measure easier to handle than the number of nodes: When combining trees under a new root, the number of leaves
				behaves additively, while the number of nodes is bigger by one node (the
				new root) than the sum. Reductions, which correspond to nodes with only
				a single successor, are being ignored in this way. For binary trees (every
				inner node as exactly two children) we have \#nds$(T) = 2\#{\rm lvs}(T)-1$. And
				finally, heuristics in a SAT solver aim at reducing the number of conflicts
				found, that is, the number of leaves.
				
				\item All leaves are treated equal (again, this corresponds to the point of view of
				the heuristics).
			\end{itemize}
			
			{\bf Adorning trees with probability distributions}
			
			Consider a finite probability space $(\Omega ,P)$, i.e., a finite set $\Omega$  of ``outcomes'' to-
			gether with a probability assignment $P : \Omega  \to [0,1]$ such that
			$\sum_{\omega \in \Omega} P(\omega ) = 1$; we
			assume furthermore that no element has zero probability ($\forall\omega \in \Omega  : P(\omega ) > 0$).
			The random variable $P^{-1}$ on the probability space $(\Omega ,P)$ assigns to every outcome $\omega$ the value $P(\omega )^{-1} $, and a trivial calculation shows that the expected value
			of $P$ is the number of outcomes:
			\begin{align}\label{equation1}
			\E(\P^{-1} ) =\sum_{\omega \in \Omega}
			P(\omega )P(\omega )^{-1} = |\Omega |.
			\end{align}
			
			So the random variable $P^{-1}$ associates to every outcome $\omega$ a guess $P^{-1} (\omega )$ on
			the (total) number of outcomes, and the expected value of these guesses is the
			true total number of all outcomes. Thus, via sampling of $P^{-1}$ we obtain an
			estimation on $|\Omega |$. In the general context this seems absurd, since the probabilities
			of outcomes are normally not given a priori, however in our application, where
			the outcomes of the probability experiment are the leaves of the search tree, we
			have natural ways at hand to calculate for each outcome its probability. We
			remark that for $ r \in \mathbb{R}_{\geq 1}$ from \ref{equation1}we get for the $r$-th moment the lower bound
			$\E((P^{-1} )^ r ) = \E((P^{-r} )) \geq |\Omega |^ r$ (by Jensen's inequality).
			
			{\defi For trees $T$ we consider {\it tree probability distributions} $\mathfrak{P}$, which
				assign to every edge $(v,w)$ in $T$ a probability $\mathfrak{P}((v,w)) \in [0,1]$ such that for all
				inner nodes $v$ we have $\sum_{w\in {\rm ds}_ T (v)} \mathfrak{P}((v,w)) = 1$, that is, the sum of outgoing
				probabilities is 1; we assume furthermore, that no edge gets a zero probability.
				We obtain the associated probability space $(\Omega_ T ,\P)$, where $\Omega_T := {\rm lvs}(T)$, that is,
				the outcomes are the leaves of $T$, which have probability
				\begin{align}
				\P(v) :=\prod^{{\rm d}(v)-1}_{i=0}\mathfrak{P}((T(i,v),T(i + 1,v))).
				\end{align}
				
				The edge-probabilities being non-zero just means that no outcome in this
				probability space has zero probability (which would mean it would be ignored).}
			
			
			{\lem For every finite rooted tree $T$ and every tree probability distribution $\mathfrak{P}$ for $T$ we have for the associated probability space $\Omega_T$ and the random
				variable $\P^{-1} : \Omega_T \to [0,1]$:
				\[
				\min \P^{-1} = \min_{v\in {\rm lvs}(T)}\P(v)^{-1} \leq \#{\rm lvs}(T) = \E(\P^{-1} ) \leq \max_{v\in {\rm lvs}(T)}
				\P(v)^{-1} = \max \P^{-1} .
				\]}
			
			Under the assumptions of lemma 1.0.9 the following assertions are equivalent:
			
			\begin{enumerate} 
				\item $\min \P^{-1} = \#{\rm lvs}(T)$.
				
				\item $\#{\rm lvs}(T) = \max \P^{-1} $.
				
				\item $\P$ is a uniform distribution (all leaves have the same probability).
			\end{enumerate}
			
			Lemma 1.0.9 opens up the following possibilities for estimating the size of a
			tree $T$, given a tree probability distribution $\mathfrak{P}$:
			
			\begin{enumerate}
				\item Upper bounding $\max \P^{-1}$ we obtain an upper bound on $\#{\rm lvs}(T)$, while
				lower bounding $\min \P^{-1}$ we obtain a lower bound on $\#{\rm lvs}(T)$.
				
				\item Estimating $\E(\P^{-1} )$ by sampling we obtain an estimation of $\#{\rm lvs}(T)$.
			\end{enumerate}
			
			
			{\lem Every finite rooted tree $T$ has exactly one tree probability distribution $\mathfrak P$ which induces a uniform probability distribution ${P}$ on the leaves, and
				this {\bf canonical tree probability distribution} $\mathfrak{CP}_ T$ is given by
				\[
				\mathfrak{CP}_T ((v,w)) =\frac{\#{\rm lvs}(T_w )}{\#{\rm lvs}(T_v )}
				\]
				for $v \in V (T)$ and $w \in {\rm ds}_ T (v)$.}
			
			The canonical tree probability distribution $ \mathfrak{CP}_{T_v}$ on a subtree $T_v$ of $T $ (for
			$v \in V (T)$) is simply obtained by restricting $\mathfrak{CP}_T $ to the edges of $T_v$ (without
			change).
			
			{\bf The variance of the estimation of the number of leaves}
			
			
			{\lem For a finite rooted tree $T$, a tree probability distribution $\mathfrak{P}$ on $T$
				and $r \in \mathbb{R}_{\geq 0}$ we can recursively compute the $r$-th moment $\E(\P^{-r} )$ of $\P^{-1}$ by
				\begin{itemize}
					\item If $T$ is trivial (i.e., $\#{\rm nds}(T) = 1$), then we have $\E(\P^{-r}_T) = 1$.
					\item Otherwise $\E(\P^{-r}_T) =\sum_{v\in {\rm ds}({\rm rt}(T))} \mathfrak{P}_T (({\rm rt}(T),v))^{1-r}\cdot \E(\P^{-r}_{T_v} )$.
				\end{itemize}}
				
				
				{\lem For a finite rooted tree $T$ and a tree probability distribution $\mathfrak{P}$
					on $T$, which fulfils $\mathfrak{P}^{-1}_T\leq \alpha \cdot \mathfrak{CP}^{-1}_T$
					for some $\alpha \in \mathbb{R}_{\geq 1}$, we have $\E(\P^{-r}_T) \leq
					\alpha^{(r-1)\cdot {\rm ht}(T)} \cdot \#{\rm lvs}(T)^r$ for all $r \in \mathbb{R}_{\geq 1}$.}
				
				{\bf The tau-method}
				
				{\defi A {\it distance} $d$ on a finite rooted tree $T$ is a map $d : E(T) \to  \mathbb{R}_{>0}$
					which assigns to every edge $(v,w)$ in $T$ a positive real number, while a {\it measure} is
					a map $\mu : V (T) \to \mathbb{R}$ such that $\Delta \mu$ is a distance, where $\Delta \mu ((v,w)) := \mu (v)-\mu (w)$.
					For a distance $ d$ we define the measures $\min \Sigma d$ and $\max \Sigma d$ on $T$, which assign
					to every vertex $v \in V (T)$ the minimal resp. maximal sum of $d$-distances over all
					paths from $v$ to some leave.}
				
				The $\tau $-function yields a canonical method to associate a tree probability distribution to a distance on a tree as follows.
				
				{\defi Consider a rooted tree $T$ together with a distance $d$. For an
					inner node $v$ of $T$ we obtain an associated branching tuple $d(v)$ modulo permutation; assuming that $T$ is ordered, i.e., we have a sorting ${\rm ds} _T (v) = \{v_1 ,\ldots,v_k\}$, we
					obtain a concrete branching tuple $d(v) := (d(v,v_1 ),\ldots,d(v,v_k))$. The associated
					tree probability distribution $\mathfrak{P}_ d$ is given by
					\[
					\mathfrak{P} _d ((v,v _i )) := \tau^\p (d(v))_ i
					\]}
				
				
				{\defi For a rooted tree $(T,r)$ with a distance $d$ let $\min\tau (d) :=
					+\infty$ and $\max\tau (d) := 1$ in case $T$ is trivial (consists just of $r$), while otherwise
					$\min\tau (d) := \min _{v\in V (T)\backslash{\rm lvs}(T)} \tau (d(v))$ and $\max\tau (d) := \max _{v\in V (T)\backslash{\rm lvs}(T)} \tau (d(v))$.}
				
				{\lem Consider a rooted tree $(T,r)$ together with a distance $d$. For the
					induced probability distribution $\P _d$ on the leaves of $T$ we have:
					
					\begin{enumerate}
						\item $(\min\tau (d))^{\min \Sigma d(r)} \leq \min \P^{-1}_d$.
						
						\item  $\max \P^{-1}_d\leq (\max\tau (d))^{\max \Sigma d(r)}$.
						
					\end{enumerate}}
					{\prf If $T$ is trivial then
						the assertion is trivial, so assume that $T$ is non-trivial. Let $\tau _0 := \min\tau (d)$.
						\[
						\min \P^{-1}_d= \min_{v\in {\rm lvs}(T)}\P_d (v)^{-1} = \min_{v\in {\rm lvs}(T)}\prod ^{d(v)-1}_{i=0}
						\mathfrak{P}_d (T(i,v),T(i + 1,v))^{-1} =
						\]
						\[
						\min_{v\in {\rm lvs}(T)}\prod^{d(v)-1}_{i=0}\tau (d(T(i,v)))^{d(T(i,v),T(i+1,v))} \leq
						\]
						\[
						\min_{v\in {\rm lvs}(T)}\prod^{d(v)-1}_{i=0}\tau^{d(T(i,v),T(i+1,v))}_0= \min_{v\in {\rm lvs}(T)}
						\tau^{\sum^{ d(v)-1}_{i=0}d(T(i,v),T(i+1,v))}_0= \tau^{ \min \Sigma d(r)}_0.
						\]\hfill$\Box$}
					
					
					{\thm Consider a rooted tree $(T,r)$. For a distance $d$ on $(T,r)$ we
						have
						\[
						(\min\tau (d))^{\min \Sigma d(r)} \leq \#{\rm lvs}(T) \leq (\max\tau (d))^{\max \Sigma d(r)}.
						\]
						And for a measure $\mu$ on $(T,r)$ which is 0 on the leaves we have
						\[
						(\min\tau (\Delta \mu ))^{\mu (r)}\leq \#{\rm lvs}(T) \leq (\max\tau (\Delta \mu ))^{\mu (r)}.
						\]}
					
					
					
					\item Algorithm and efficient implementation of $\tau(a,b)$ XXX
				\end{enumerate}
				
				
				
			\end{document}